from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import pandas as pd
import time
import sys
import os
import argparse


"""
This script is used to generate ROC curves, AUC, and confusion matrices based
on a variety of user inputs.

It is designed to be specifically utilized with the other scripts in this
project which generate the prerequisite CSVs for this script.
Namely, those scripts are:
main.py (utilized by the Sybil container image).
cleanup_nlst_for_sybil.py (requires nlst clinical data, either via CDAS or the
Cancer Imaging Archive).
"""

# Constants
N_PREDICTION_YEARS = 6

OPERATOR_DICT = {
    'e'     :   '==',
    'g'     :   '>',
    'l'     :   '<',
    'ge'    :   '>=',
    'le'    :   '<='
}

DEFAULT_CUTOFFS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

def main():
    print("Sybil Evaluation")

    # ArgParse library is used to manage command line arguments.
    parser = argparse.ArgumentParser(
        epilog="Example: sybil_eval.py \
        path/to/actual.csv path/to/prediction.csv -o output_dir \
        -f gender:2:e race:2:e -c 0.25 0.5 0.75"
    )
    parser.add_argument("actual", help="a CSV file generated by \
        cleanup_nlst_for_sybil.py which contains the actual values regarding \
        the presence of cancer n years after a given CT scan.")
    parser.add_argument("prediction", help="a CSV file generated by main.py \
        (utilized by Sybil container image) which contains the prediction \
        values generated by Sybil regarding the probability of cancer n years \
        after a given CT scan.")
    parser.add_argument('-o', "--outdir", help="A directory in which to \
        generate the output. \
        Default: script current working directory.",
        default=os.getcwd())
    parser.add_argument('-f', "--filters", help="Any number of filters to \
        apply to the data, formated as such: property_name:value:operator, \
        e.g. race:2:e. Operator options: \
        e -> equal, g -> greater than, l -> less than, \
        ge -> greater than or equal to, le -> less than or equal to. \
        Default: no filters.", 
        nargs='+', default=[])
    parser.add_argument('-c', '--cutoffs', help="Any number of probability \
        cutoffs to be used for the generation of multiple confusion matrices. \
        Default: " + str(DEFAULT_CUTOFFS), type=float,
        nargs='+', default=DEFAULT_CUTOFFS)
    args = parser.parse_args()
    print("Actual:", args.actual)
    print("Prediction:", args.prediction)
    print("Output directory:", args.outdir)
    print("Filters:", args.filters)
    print("Cutoffs:", args.cutoffs)

    # Read in CSVs
    actual = pd.read_csv(args.actual)
    prediction = pd.read_csv(args.prediction)

    # Filter the actual CSV
    if len(args.filters) > 0:
        actual_filtered = parse_filters(actual, args.filters)
    else:
        actual_filtered = actual

    # Initialize new actual dataframe
    actual_aligned = []
    prediction_aligned = []

    # Iterate through the filtered CSV
    for index, row in actual_filtered.iterrows():
        query_str = ""
        # Find entries with the same PID
        query_str += "pid == " + str(row["pid"]) + " and "
        # Find entries with the same screening year
        query_str += "study_yr == " + str(row["study_yr"])
        queried_prediction = prediction.query(query_str)
        n_predictions = queried_prediction.shape[0]

        # There are multiple CT scans per individual patient per study year.
        # Because of this, the actual data and prediction data don't align.
        # Thus, repeated actual data columns must be generated to match the
        # prediction data.
        actual_values = []
        for year in range(1,N_PREDICTION_YEARS+1):
            column_name = "canc_yr" + str(year)
            actual_value = row[column_name]
            actual_values.append(actual_value)
        for repeat in range(n_predictions):
            actual_aligned.append(actual_values)

        # Add the prediction probability values to the aligned DataFrame.
        for index in range(n_predictions):
            prediction_values = []
            for year in range(1,N_PREDICTION_YEARS+1):
                column_name = "pred_yr" + str(year)
                prediction_value = queried_prediction.iloc[index][column_name]
                prediction_values.append(prediction_value)
            prediction_aligned.append(prediction_values)
    
    # Create DataFrames
    # These dataframes can now be compared-
    # year1 of the actual aligned df can be compared with year1 of the
    # prediction aligned df, year2 with year2, and so on.
    column_names = ["year" + str(i) for i in range(1,N_PREDICTION_YEARS+1)]
    actual_aligned_df = pd.DataFrame(
        actual_aligned, columns = column_names
    )
    prediction_aligned_df = pd.DataFrame(
        prediction_aligned, columns = column_names
    )

    # Create output directory named based on filters.
    output_directory = args.outdir + '/' + generate_dir_name(args.filters)
    if not os.path.exists(output_directory):
        os.mkdir(output_directory)

    # Execute function to generate multi-ROC curve, generates PNG.
    generate_multi_roc(
        actual_aligned_df,
        prediction_aligned_df,
        output_directory
    )

    # Execute function to generate multiple confusion matrices, generates one
    # CSV file per prediction year.
    generate_confusion_matrices(
        actual_aligned_df,
        prediction_aligned_df,
        output_directory,
        args.cutoffs
    )

def generate_dir_name(filters: list[str]) -> str:
    # This function generates the name of the output directory depending on the
    # filters used in the command line arguments.
    output = "sybil_eval"
    if len(filters) == 0:
        return output + "_no_filters"
    for index, filt in enumerate(filters):
        output += '_'
        filter_clean = filt.replace(':', '')
        filter_clean = filter_clean.replace('_', '')
        output += filter_clean
    return output


def parse_filters(df: pd.DataFrame, filters: list[str]) -> pd.DataFrame:
    # This function applies user-selected filters to given DataFrame.
    # Example: race:2:e will return a DataFrame which contains only rows where
    # the associated patient has a race=2 (Black).
    # EXAMPLE BREAKDOWN:
    # race:2:e
    # race = the column to use for filtering
    # 2 = the value used for comparison
    # e = the chosen comparison operator, in this case 'equal to'
    # The filter therefore selects only individuals whose race is equal to 2.

    query_str = ""
    for index in range(len(filters)):
        parse_list = filters[index].split(':')
        appended_str = ""
        
        # Add property name
        # Ensure valid property name
        if parse_list[0] not in df.columns:
            columns_str = ', '.join(df.columns)
            raise Exception("Invalid property name found in filter " \
                f"{filters[index]}: {parse_list[0]}. " \
                "\nPlease select a property name from the following list: " \
                f"\n{columns_str}")
        appended_str += parse_list[0] + " "

        # Add operator
        # Ensure valid operator
        if parse_list[2] not in OPERATOR_DICT.keys():
            operators_str = ', '.join(OPERATOR_DICT.keys())
            raise Exception("Invalid operator found in filter " \
                f"({filters[index]}): {parse_list[2]}. " \
                "\nPlease select an operator from the following list: " \
                f"\n{operators_str}")
        appended_str += OPERATOR_DICT[parse_list[2]] + " "

        # Add value
        appended_str += parse_list[1]

        # Add 'and'
        if index != len(filters) - 1:
            appended_str += " and "

        query_str += appended_str
    print(query_str) ### TODO: remove
    return df.query(query_str)      

def generate_multi_roc(actual, prediction, out_dir):
    # This function uses actual and prediction values to create a multi-ROC
    # curve PNG image, which it then stores in a specified output directory.
    # The generated image is labeled such that each curve is identified by year,
    # and area under curve (AUC) value is provided for each curve.
    
    print("Generating Multi-ROC curve...")
    
    plt.figure(figsize = (5, 5), dpi = 100)
    for year in actual:
        current_actual = actual[year].tolist()
        current_prediction = prediction[year].tolist()
        fpr, tpr, threshold = roc_curve(current_actual, current_prediction)
        # Calculate Area Under Curve (AUC), round to nearest 5 decimal points.
        roc_auc = round(auc(fpr, tpr), 5)

        plt.plot(fpr, tpr, linestyle = "-",
            label = f"{year}: AUC = {roc_auc}")
        # TODO: test plot generation

    plt.xlabel("1 - Specificity")
    plt.ylabel("Sensitivity")
    plt.title("Sybil Performance", loc = "center")
    plt.legend(loc = "lower right")

    file_name = "multi_roc.png"
    plt.savefig(out_dir + "/" + file_name)

def generate_confusion_matrices(actual, prediction, out_dir, cutoffs):
    # This function uses actual and prediction values to create multiple
    # confusion matrices
    # TODO: Confusion matrix requires:
    # True positive, true negative, false positive, false negative.
    # Create such a table for every probability cutoff.
    
    print("Generating confusion matrices...")

    for year in actual:
        csv_name = "confusion_matrices_" + year + ".csv"
        with open(out_dir + "/" + csv_name, 'w') as current_csv:
            for cutoff in cutoffs:
                current_csv.write(f"Probability cutoff,=,{cutoff}\n")
                
                tp = 0 # True Positive
                tn = 0 # True Negative
                fp = 0 # False Positive
                fn = 0 # False Negative
                
                # Count TP, TN, FP, FN
                for index, current_actual in enumerate(actual[year]):
                    current_prediction = prediction[year][index]
                    guess_positive: bool = current_prediction >= cutoff
                    if   current_actual == 1 and     guess_positive:
                        tp += 1
                    elif current_actual == 0 and not guess_positive:
                        tn += 1
                    elif current_actual == 0 and     guess_positive:
                        fp += 1
                    elif current_actual == 1 and not guess_positive:
                        fn += 1
                total = tp+tn+fp+fn

                # Write confusion matrix
                current_csv.write(",actual_positive,actual_negative\n")
                current_csv.write(f"prediction_positive,{tp},{fp}\n")
                current_csv.write(f"prediction_negative,{fn},{tn}\n")

                # Calculate descriptive values
                sensitivity = round(tp/(tp+fn), 5) if (tp+fn) != 0 else "DIV0"
                specificity = round(tn/(tn+fp), 5) if (tn+fp) != 0 else "DIV0"

                accuracy = round((tp+tn)/total, 5) if total != 0 else "DIV0"
                # Positive Predictive Value
                ppv = round(tp/(tp+fp), 5) if (tp+fp) != 0 else "DIV0"
                # Negative Predictive Value
                npv = round(tn/(tn+fn), 5) if (tn+fn) != 0 else "DIV0"

                # Write values
                current_csv.write(f"Sensitivity,=,{sensitivity}\n")
                current_csv.write(f"Specificity,=,{specificity}\n")
                current_csv.write(f"Accuracy,=,{accuracy}\n")
                current_csv.write(f"PPV,=,{ppv}\n")
                current_csv.write(f"NPV,=,{npv}\n\n")

start = time.perf_counter()
main()
end = time.perf_counter()
print(f"{sys.argv[0]} Completed in {end - start:0.4f} seconds.")

